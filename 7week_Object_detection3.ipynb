{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 지금까지의 방법은 two-stage\n",
    "\n",
    "## region proposals\n",
    "- 관심있을 만한 영역을 추천하는 방식\n",
    "- 영역이 무엇인지 예측하는 것이 R-CNN \n",
    "    - convolution은 할 수 있지만 크기 문제\n",
    "\n",
    "<br>\n",
    "\n",
    "- 최근에는 single stage 방법이 two stage를 거의 따라잡음\n",
    "- faster-RCNN은 1초 2장, yolo는 1초 35장\n",
    "- single stage는 일정 크기 이하는 잘 찾지 못함\n",
    "\n",
    "## bounding box Regressor\n",
    "- 박스의 차이를 linear regression으로 함.\n",
    "\n",
    "- Fast R-CNN부터 multi task 방식을 사용하기 시작함\n",
    "    - 일반적인 loss를 사용하는 건 잘못됨\n",
    "    - 특징과 영역이 convolution으로 인해 잘 나옴\n",
    "- 여전히 selective search사용\n",
    "\n",
    "## Faster R-CNN\n",
    "- selective 빼고 neural network사용\n",
    "- **anchor box** 이용\n",
    "    - 다양하게 참조하는 pyramid?\n",
    "    \n",
    "Faster R-CNN: Towards Real-Time Object\n",
    "Detection with Region Proposal Networks\n",
    "\n",
    "### RPN\n",
    "- sliding window를 하는데 가장 있을만한 size로 9개 형태 가정\n",
    "- 256 -> ZFNet\n",
    "- 9개 각각에 대해 배경인지 아닌지 판단 -> 좌표값 4k개\n",
    "    - 특정 모양 영역이 어느 정도 겹치면 물체가 있다고 가정(region proposal)\n",
    "- k개의 anchor box\n",
    "- 학습시킬 때 feature map에서 배경인지 아닌지\n",
    "- YOLO의 grid방식과 비슷\n",
    "    \n",
    "\n",
    "- kaggle\n",
    "https://www.kaggle.com/kishor1210/train-faster-rcnn-using-keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pprint\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "from optparse import OptionParser\n",
    "import pickle\n",
    "import math\n",
    "import cv2\n",
    "import copy\n",
    "from matplotlib import pyplot as plt\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.optimizers import Adam, SGD, RMSprop\n",
    "from tensorflow.keras.layers import Flatten, Dense, Input, Conv2D, MaxPooling2D, Dropout\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, GlobalMaxPooling2D, TimeDistributed\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import initializers, regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        # Print the process or not\n",
    "        self.verbose = True\n",
    "\n",
    "        # Name of base network\n",
    "        self.network = 'vgg'\n",
    "\n",
    "        # Setting for data augmentation\n",
    "        self.use_horizontal_flips = False\n",
    "        self.use_vertical_flips = False\n",
    "        self.rot_90 = False\n",
    "\n",
    "        # Anchor box scales\n",
    "    # Note that if im_size is smaller, anchor_box_scales should be scaled\n",
    "    # Original anchor_box_scales in the paper is [128, 256, 512]\n",
    "        self.anchor_box_scales = [64, 128, 256] \n",
    "\n",
    "        # Anchor box ratios\n",
    "        self.anchor_box_ratios = [[1, 1], [1./math.sqrt(2), 2./math.sqrt(2)], [2./math.sqrt(2), 1./math.sqrt(2)]]\n",
    "\n",
    "        # Size to resize the smallest side of the image\n",
    "        # Original setting in paper is 600. Set to 300 in here to save training time\n",
    "        self.im_size = 300\n",
    "\n",
    "        # image channel-wise mean to subtract\n",
    "        self.img_channel_mean = [103.939, 116.779, 123.68]\n",
    "        self.img_scaling_factor = 1.0\n",
    "\n",
    "        # number of ROIs at once\n",
    "        self.num_rois = 4\n",
    "\n",
    "        # stride at the RPN (this depends on the network configuration)\n",
    "        self.rpn_stride = 16\n",
    "\n",
    "        self.balanced_classes = False\n",
    "\n",
    "        # scaling the stdev\n",
    "        self.std_scaling = 4.0\n",
    "        self.classifier_regr_std = [8.0, 8.0, 4.0, 4.0]\n",
    "\n",
    "        # overlaps for RPN\n",
    "        self.rpn_min_overlap = 0.3\n",
    "        self.rpn_max_overlap = 0.7\n",
    "\n",
    "        # overlaps for classifier ROIs\n",
    "        self.classifier_min_overlap = 0.1\n",
    "        self.classifier_max_overlap = 0.5\n",
    "\n",
    "        # placeholder for the class mapping, automatically generated by the parser\n",
    "        self.class_mapping = None\n",
    "\n",
    "        self.model_path = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5개 함수 구현하는 게 customizing의 핵심"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 3), dtype=uint8, numpy=array([[171, 167, 171]], dtype=uint8)>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from skimage.io import imread\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.GlobalAvgPool2D()\n",
    "])\n",
    "im = imread('1.jpg')\n",
    "model(im[np.newaxis])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1350, 900, 3)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "im[np.newaxis].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.applications."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py37] *",
   "language": "python",
   "name": "conda-env-py37-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
